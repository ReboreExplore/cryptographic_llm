{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ec3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu = os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c34da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 12:40:37.420051: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-07 12:40:37.507357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 12:40:39.018298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer,pipeline, logging, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191fbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930b7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a fine-tuned AI model who is a math genious. \n",
    "You can solve simple to moderate level mathematics problems. \n",
    "Follow a chain of thought approach while answering. Answer in brief. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a128e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f954e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir=\"/projects/barman/cache\",\n",
    "#     use_flash_attention_2=use_flash_attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a6897b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8fb9257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Can you explain Caeser Cipher with an example?\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   Sure, I'd be happy to explain the Caesar Cipher!\n",
      " Unterscheidung:\n",
      "\n",
      "The Caesar Cipher is a simple encryption technique that replaces each letter of a message with a letter a fixed number of positions down the alphabet. For example, if we use a shift of 2, the letter \"a\" becomes \"c\", \"b\" becomes \"d\", \"c\" becomes \"e\", and so on.\n",
      "\n",
      "Here's an example of how the Caesar Cipher works:\n",
      "\n",
      "Original message: HELLO\n",
      "Shift: 2\n",
      "Encrypted message: CJLG\n",
      "\n",
      "To encrypt the message \"HELLO\", we shift each letter 2 positions down the alphabet, so the first letter \"H\" becomes \"C\", the second letter \"E\" becomes \"G\", and so on.\n",
      "\n",
      "Decryption is simply the reverse of encryption, where we shift each letter back up the alphabet by the same number of positions. In this case, the decrypted message is \"HELLO\".\n",
      "\n",
      "Here's another example:\n",
      "\n",
      "Original message: THE QUICK BROWN FOX\n",
      "Shift: 3\n",
      "Encrypted message: KCJG UBXF\n",
      "\n",
      "In this example, the shift is 3, so the first letter \"T\" becomes \"K\", the second letter \"H\" becomes \"G\", and so on. The decrypted message is \"THE QUICK BROWN FOX\".\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Can you explain Caeser Cipher with an example?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afff6b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Can you explain Caeser Cipher with an example?\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   Of course! The Caesar Cipher is a simple encryption technique that replaces each letter of a message with a letter a fixed number of positions down the alphabet. Here's how it works:\n",
      "\n",
      "1. Choose a number of positions to shift the letters by (e.g. 3). This is called the \"cipher shift.\"\n",
      "2. Write out the message you want to encrypt, using each letter as is.\n",
      "3. To encrypt each letter, move it down the alphabet by the number of positions you chose in step 1. For example, if the message is \"HELLO\", and you chose a cipher shift of 3, the encrypted message would be \"KLLO\".\n",
      "\n",
      "Here's an example of how to encrypt the message \"HELLO\" using a cipher shift of 3:\n",
      "\n",
      "H -> K (position 3)\n",
      "E -> F (position 6)\n",
      "L -> L (position 3)\n",
      "L -> K (position 6)\n",
      "O -> O (position 0)\n",
      "\n",
      "So the encrypted message is \"KLLO\".\n",
      "\n",
      "To decrypt the message, you simply reverse the process by moving each letter up the alphabet by the same number of positions. For example, to decrypt the encrypted message \"KLLO\", you would move each letter up the alphabet by 3 positions, like this:\n",
      "\n",
      "K -> H (position 0)\n",
      "L -> E (position 3)\n",
      "L -> L (position 6)\n",
      "O -> O (position 0)\n",
      "\n",
      "And the original message \"HELLO\" is restored!\n",
      "\n",
      "I hope this helps you understand how the Caesar Cipher works! Let me know if you have any questions or if you'd like to try encrypting a message using a different cipher shift.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Can you explain Caeser Cipher with an example?\"\n",
    "SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> <<SYS>> {SYSTEM_PROMPT} <</SYS>> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4f09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Can you explain Caeser Cipher with an example? \n",
      "For example: If the given world is HELLO and shift value is 2 then each letter should sshift by 2 towards the right from the English Alphabet, \n",
      "i.e. H becomes J, E becomes G, L becomes N, L becomes N, O becomes Q. Similarly, the word ART with a shift of -4 becomes WNP \n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   Of course! I'd be happy to explain the Caesar Cipher using an example.\n",
      "\n",
      "The Caesar Cipher is a simple encryption technique that replaces each letter of a message with a letter a fixed number of positions down the alphabet. The encryption is done by shifting each letter of the message by a certain number of positions, either towards the right (positive shift) or towards the left (negative shift).\n",
      "\n",
      "Let's use the example you provided:\n",
      "\n",
      "Original message: HELLO\n",
      "Shift value: 2\n",
      "\n",
      "To encrypt the message \"HELLO\", we shift each letter by 2 positions towards the right in the alphabet:\n",
      "\n",
      "Encrypted message: JGNO\n",
      "\n",
      "Here's how the encryption works:\n",
      "\n",
      "1. Start with the original message \"HELLO\".\n",
      "2. Shift each letter by the shift value (2 in this case) towards the right in the alphabet.\n",
      "3. The resulting letters are the encrypted message.\n",
      "\n",
      "So, in this case, the encrypted message is \"JGNO\".\n",
      "\n",
      "Now, let's try the same example with a negative shift value. Suppose we want to encrypt the message \"ART\" with a shift value of -4. Here's how it would work:\n",
      "\n",
      "Original message: ART\n",
      "Shift value: -4\n",
      "\n",
      "To encrypt the message \"ART\", we shift each letter by 4 positions towards the left in the alphabet:\n",
      "\n",
      "Encrypted message: WNP\n",
      "\n",
      "As you can see, the encryption is simply a matter of shifting the letters of the original message by a fixed number of positions in the alphabet. The shift value determines the direction of the shift, and the resulting encrypted message is the result.\n",
      "\n",
      "I hope this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Does Few Shot learning helps in any way ?\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"\"\"Can you explain Caeser Cipher with an example? \n",
    "For example: If the given world is HELLO and shift value is 2 then each letter should sshift by 2 towards the right from the English Alphabet, \n",
    "i.e. H becomes J, E becomes G, L becomes N, L becomes N, O becomes Q. Similarly, the word ART with a shift of -4 becomes WNP. \"\"\"\n",
    "SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> <<SYS>> {SYSTEM_PROMPT} <</SYS>> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce42982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: If the given world is HELLO and shift value is 2 then each letter should sshift by 2 towards the right from the English Alphabet, i.e. H becomes J, E becomes G, L becomes N, L becomes N, O becomes Q. Similarly, the word ART with a shift of -4 becomes WNP.\n",
      "Encoded sentence: [1, 960, 278, 2183, 3186, 338, 17714, 2208, 29949, 322, 9500, 995, 338, 29871, 29906, 769, 1269, 5497, 881, 269, 10889, 491, 29871, 29906, 7113, 278, 1492, 515, 278, 4223, 838, 17416, 29892, 474, 29889, 29872, 29889, 379, 7415, 435, 29892, 382, 7415, 402, 29892, 365, 7415, 405, 29892, 365, 7415, 405, 29892, 438, 7415, 660, 29889, 20175, 29892, 278, 1734, 9033, 29911, 411, 263, 9500, 310, 448, 29946, 7415, 399, 25500, 29889]\n",
      "Token ID 1 --> <s>\n",
      "Token ID 960 --> If\n",
      "Token ID 278 --> the\n",
      "Token ID 2183 --> given\n",
      "Token ID 3186 --> world\n",
      "Token ID 338 --> is\n",
      "Token ID 17714 --> HE\n",
      "Token ID 2208 --> LL\n",
      "Token ID 29949 --> O\n",
      "Token ID 322 --> and\n",
      "Token ID 9500 --> shift\n",
      "Token ID 995 --> value\n",
      "Token ID 338 --> is\n",
      "Token ID 29871 --> \n",
      "Token ID 29906 --> 2\n",
      "Token ID 769 --> then\n",
      "Token ID 1269 --> each\n",
      "Token ID 5497 --> letter\n",
      "Token ID 881 --> should\n",
      "Token ID 269 --> s\n",
      "Token ID 10889 --> shift\n",
      "Token ID 491 --> by\n",
      "Token ID 29871 --> \n",
      "Token ID 29906 --> 2\n",
      "Token ID 7113 --> towards\n",
      "Token ID 278 --> the\n",
      "Token ID 1492 --> right\n",
      "Token ID 515 --> from\n",
      "Token ID 278 --> the\n",
      "Token ID 4223 --> English\n",
      "Token ID 838 --> Al\n",
      "Token ID 17416 --> phabet\n",
      "Token ID 29892 --> ,\n",
      "Token ID 474 --> i\n",
      "Token ID 29889 --> .\n",
      "Token ID 29872 --> e\n",
      "Token ID 29889 --> .\n",
      "Token ID 379 --> H\n",
      "Token ID 7415 --> becomes\n",
      "Token ID 435 --> J\n",
      "Token ID 29892 --> ,\n",
      "Token ID 382 --> E\n",
      "Token ID 7415 --> becomes\n",
      "Token ID 402 --> G\n",
      "Token ID 29892 --> ,\n",
      "Token ID 365 --> L\n",
      "Token ID 7415 --> becomes\n",
      "Token ID 405 --> N\n",
      "Token ID 29892 --> ,\n",
      "Token ID 365 --> L\n",
      "Token ID 7415 --> becomes\n",
      "Token ID 405 --> N\n",
      "Token ID 29892 --> ,\n",
      "Token ID 438 --> O\n",
      "Token ID 7415 --> becomes\n",
      "Token ID 660 --> Q\n",
      "Token ID 29889 --> .\n",
      "Token ID 20175 --> Similarly\n",
      "Token ID 29892 --> ,\n",
      "Token ID 278 --> the\n",
      "Token ID 1734 --> word\n",
      "Token ID 9033 --> AR\n",
      "Token ID 29911 --> T\n",
      "Token ID 411 --> with\n",
      "Token ID 263 --> a\n",
      "Token ID 9500 --> shift\n",
      "Token ID 310 --> of\n",
      "Token ID 448 --> -\n",
      "Token ID 29946 --> 4\n",
      "Token ID 7415 --> becomes\n",
      "Token ID 399 --> W\n",
      "Token ID 25500 --> NP\n",
      "Token ID 29889 --> .\n"
     ]
    }
   ],
   "source": [
    "# Is there a problem with the llama tokenizer?\n",
    "# Test tokenizer on the prompt\n",
    "\n",
    "sentence = \"If the given world is HELLO and shift value is 2 then each letter should sshift by 2 towards the right from the English Alphabet, i.e. H becomes J, E becomes G, L becomes N, L becomes N, O becomes Q. Similarly, the word ART with a shift of -4 becomes WNP.\"\n",
    "encoded_output = tokenizer.encode(sentence)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded sentence: {encoded_output}\")\n",
    "\n",
    "# Verify what each token ID correlates to\n",
    "for token in encoded_output:\n",
    "    print(f\"Token ID {token} --> {tokenizer.decode(token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f5ed34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Define plaintext and ciphertext in cryptography.\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   In cryptography, plaintext refers to the original, unencrypted message or data that is intended to be protected from unauthorized access. This is the message that is easily readable and understandable by its intended recipient.\n",
      "\n",
      "On the other hand, ciphertext is the encrypted version of the plaintext message. It is the scrambled, encoded, or encrypted message that can only be decrypted and read by someone who has the appropriate decryption key or algorithm. In other words, ciphertext is the encrypted message that is not easily readable or understandable by anyone who does not have the proper authorization to access it.\n",
      "\n",
      "To illustrate the difference between plaintext and ciphertext, let's consider an example:\n",
      "\n",
      "Suppose you want to send an email to a colleague, and you want to ensure that only your colleague can read the contents of the email. In this case, the email contents are the plaintext message, and the encrypted version of the email is the ciphertext.\n",
      "\n",
      "Here's how the process might work:\n",
      "\n",
      "1. You type the plaintext message (the contents of the email) into your email client.\n",
      "2. You encrypt the plaintext message using an encryption algorithm (e.g., AES) and a decryption key (which is only known to you and your colleague).\n",
      "3. The encrypted message is the ciphertext.\n",
      "4. You send the ciphertext to your colleague via email.\n",
      "5. Your colleague can only decrypt the ciphertext using the decryption key, which only they possess.\n",
      "6. Once decrypted, the original plaintext message is revealed.\n",
      "\n",
      "In summary, plaintext is the original, unencrypted message, while ciphertext is the encrypted version of that message that can only be decrypted by someone with the proper authorization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Define plaintext and ciphertext in cryptography.\"\n",
    "SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> <<SYS>> {SYSTEM_PROMPT} <</SYS>> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e71ae3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Define plaintext and ciphertext in cryptography.\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   In cryptography, plaintext and ciphertext are two fundamental concepts used to describe the process of encryption and decryption.\n",
      "\n",
      "Plaintext refers to the original, unencrypted text or data that is intended to be protected from unauthorized access or interception. In other words, it is the original message or data that is meant to be encrypted. Plaintext is the \"clear\" or \"unencoded\" version of the data, and it is the starting point for the encryption process.\n",
      "\n",
      "Ciphertext, on the other hand, refers to the encrypted version of the plaintext. It is the text or data that has been transformed through an encryption algorithm, making it unreadable to anyone who does not have the appropriate decryption key or method. In other words, ciphertext is the \"encoded\" or \"encrypted\" version of the plaintext, and it is the result of the encryption process.\n",
      "\n",
      "To illustrate the difference between plaintext and ciphertext, consider the following example:\n",
      "\n",
      "Suppose you want to send an email to a friend that contains sensitive information, such as your credit card number. The email is in plaintext, meaning it contains the actual credit card number in plain sight. To protect the information from being intercepted or accessed by unauthorized parties, you use an encryption algorithm to transform the plaintext email into ciphertext. The resulting ciphertext can only be decrypted and read by someone who has the appropriate decryption key or method.\n",
      "\n",
      "In summary, plaintext is the original, unencrypted data, while ciphertext is the encrypted version of that data that is protected from unauthorized access.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Define plaintext and ciphertext in cryptography.\"\n",
    "# SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8c81a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "What is the Vigenère cipher? Answer is brief.\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   The Vigenère cipher is a polyalphabetic substitution cipher that uses a series of Caesar ciphers based on a keyword. Each letter of the plaintext is replaced by a letter a fixed number of positions down the alphabet, and the same substitution is applied to each letter of the plaintext. The number of positions is determined by the keyword.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is the Vigenère cipher? Answer is brief.\"\n",
    "# SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ffefb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "What is the main difference between substitution-permutation networks (SPN) and Feistel networks in block cipher design? Answer very briefly\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   Substitution-permutation networks (SPN) and Feistel networks are both block cipher structures used for encryption. Unterscheidung between them lies in the way they combine substitution and permutation operations:\n",
      "\n",
      "* SPN: Substitution operations are applied first, followed by permutation operations.\n",
      "* Feistel network: Permutation operations are applied first, followed by substitution operations.\n",
      "\n",
      "In simpler terms, SPNs perform substitution and permutation operations in a particular order, while Feistel networks perform permutation and substitution operations in a mixed order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is the main difference between substitution-permutation networks (SPN) and Feistel networks in block cipher design? Answer very briefly\"\n",
    "# SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de90cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Is 989797798979897988 divisible by 3? Justify your answer in brief\n",
      "\n",
      "Generated Response llama 2 7b chat\n",
      "   No, 989797798979897988 is not divisible by 3. everybody.\n",
      "\n",
      "To check if a number is divisible by 3, we can simply divide the number by 3 and see if the result is a whole number. In this case, dividing 989797798979897988 by 3 results in a remainder of 0, which means that 3 does not divide into the number. Therefore, 989797798979897988 is not divisible by 3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Is 989797798979897988 divisible by 3? Justify your answer in brief\"\n",
    "# SYSTEM_PROMPT = \"You are a cryptographic expert and you can easily solve medium to higher difficulty cryptographic problems. Please follow my instructions and solve the problem using Chain of thought approach.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s> [INST] {prompt} [/INST]\")\n",
    "\n",
    "print(f\"Input:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response llama 2 7b chat\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96454f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Is 989797798979897988 divisible by 3?\n",
      "Encoded sentence: [1, 1317, 29871, 29929, 29947, 29929, 29955, 29929, 29955, 29955, 29929, 29947, 29929, 29955, 29929, 29947, 29929, 29955, 29929, 29947, 29947, 8572, 1821, 491, 29871, 29941, 29973]\n",
      "Token ID 1 --> <s>\n",
      "Token ID 1317 --> Is\n",
      "Token ID 29871 --> \n",
      "Token ID 29929 --> 9\n",
      "Token ID 29947 --> 8\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29955 --> 7\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29955 --> 7\n",
      "Token ID 29955 --> 7\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29947 --> 8\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29955 --> 7\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29947 --> 8\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29955 --> 7\n",
      "Token ID 29929 --> 9\n",
      "Token ID 29947 --> 8\n",
      "Token ID 29947 --> 8\n",
      "Token ID 8572 --> divis\n",
      "Token ID 1821 --> ible\n",
      "Token ID 491 --> by\n",
      "Token ID 29871 --> \n",
      "Token ID 29941 --> 3\n",
      "Token ID 29973 --> ?\n"
     ]
    }
   ],
   "source": [
    "# Is the llama tokenizer at fault?\n",
    "# Test tokenizer on the prompt\n",
    "\n",
    "sentence = \"Is 989797798979897988 divisible by 3?\"\n",
    "encoded_output = tokenizer.encode(sentence)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded sentence: {encoded_output}\")\n",
    "\n",
    "# Verify what each token ID correlates to\n",
    "for token in encoded_output:\n",
    "    print(f\"Token ID {token} --> {tokenizer.decode(token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c94ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-llm-v2-python",
   "language": "python",
   "name": "crypto-llm-v2-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
