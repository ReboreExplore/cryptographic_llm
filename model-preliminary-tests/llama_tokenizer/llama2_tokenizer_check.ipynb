{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Login for once in terminal and give the hugging face read credential\n",
    "# !huggingface-cli login\n",
    "# !huggingface-cli download \\\n",
    "# --local-dir=/scratch/users/barman/cryptollm_tests/llama_tokenizer meta-llama/Llama-2-7b-hf \\\n",
    "# tokenizer.model tokenizer.json tokenizer_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dba8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cryptollm-dv-1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9065ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer_path = \"llama_tokenizer\" # Directory with files\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64c50a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Tokenizer Details ####################\n",
      "\n",
      "Llama2 tokenizer overview: LlamaTokenizerFast(name_or_path='llama_tokenizer', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "\n",
      "Llama2 Vocabulary Size: 32000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check what type of tokens exits\n",
    "# Constants\n",
    "stdout_padding = \"#\" * 20\n",
    "# Confirm vocabulary size\n",
    "print(f\"{stdout_padding} Llama2 Tokenizer Details {stdout_padding}\\n\")\n",
    "print(f\"Llama2 tokenizer overview: {tokenizer}\\n\")\n",
    "print(f\"Llama2 Vocabulary Size: {len(tokenizer.get_vocab().keys())}\\n\")\n",
    "# print(f\"{stdout_padding} End of Llama2 Tokenizer Details {stdout_padding}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffbf85",
   "metadata": {},
   "source": [
    "### Note\n",
    "- Vocabulary Size: 32000\n",
    "- Model Maximum Length: 1000000000000000019884624838656\n",
    "- Padding Side: Right (Important for finetuning)\n",
    "- Truncation Side: Right\n",
    "- Special Tokens:\n",
    "    - Beginning of Sentence Token: \\<s>\n",
    "    - End of Sentence Token: \\</s>\n",
    "    - Unknown Token: \\<unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9313a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Special Tokens ####################\n",
      "\n",
      "Token ID for the special token <unk>: 0\n",
      "Encoded <unk> becomes: [1, 0]\n",
      "\n",
      "Token ID for the special token <s>: 1\n",
      "Encoded <s> becomes: [1, 1]\n",
      "\n",
      "Token ID for the special token </s>: 2\n",
      "Encoded </s> becomes: [1, 2]\n",
      "\n",
      "#################### End of Llama2 Special Tokens ####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify token IDs for Llama2 special tokens\n",
    "print(f\"{stdout_padding} Llama2 Special Tokens {stdout_padding}\\n\")\n",
    "\n",
    "UNK = \"<unk>\" # Unknown token\n",
    "BOS, EOS = \"<s>\", \"</s>\" # Begin of sequnece and end of sequence tokens\n",
    "\n",
    "special_tokens = [UNK, BOS, EOS]\n",
    "\n",
    "for token in special_tokens:\n",
    "    print(f'Token ID for the special token {token}: {tokenizer.get_vocab()[token]}')\n",
    "    print(f'Encoded {token} becomes: {tokenizer.encode(token)}\\n')\n",
    "\n",
    "print(f\"{stdout_padding} End of Llama2 Special Tokens {stdout_padding}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68561dc6",
   "metadata": {},
   "source": [
    "### Note\n",
    "- The \\<s> token represents the beginning of a sequence, so when we convert any sequence to its tokenized form, we'll prepend it with the beginning of sequence token, \\<s> or 1.\n",
    "- But note that there is no end of token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c819c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Prompt Symbols ####################\n",
      "\n",
      "Encoded '[INST]' becomes: [1, 518, 25580, 29962]\n",
      "\tToken ID 1 --> '<s>'\n",
      "\tToken ID 518 --> '['\n",
      "\tToken ID 25580 --> 'INST'\n",
      "\tToken ID 29962 --> ']'\n",
      "Encoded '[/INST]' becomes: [1, 518, 29914, 25580, 29962]\n",
      "\tToken ID 1 --> '<s>'\n",
      "\tToken ID 518 --> '['\n",
      "\tToken ID 29914 --> '/'\n",
      "\tToken ID 25580 --> 'INST'\n",
      "\tToken ID 29962 --> ']'\n",
      "Encoded '<<SYS>>\\n' becomes: [1, 3532, 14816, 29903, 6778, 13]\n",
      "\tToken ID 1 --> '<s>'\n",
      "\tToken ID 3532 --> '<<'\n",
      "\tToken ID 14816 --> 'SY'\n",
      "\tToken ID 29903 --> 'S'\n",
      "\tToken ID 6778 --> '>>'\n",
      "\tToken ID 13 --> '\\n'\n",
      "Encoded '\\n<<SYS>>\\n\\n' becomes: [1, 29871, 13, 9314, 14816, 29903, 6778, 13, 13]\n",
      "\tToken ID 1 --> '<s>'\n",
      "\tToken ID 29871 --> ''\n",
      "\tToken ID 13 --> '\\n'\n",
      "\tToken ID 9314 --> '<<'\n",
      "\tToken ID 14816 --> 'SY'\n",
      "\tToken ID 29903 --> 'S'\n",
      "\tToken ID 6778 --> '>>'\n",
      "\tToken ID 13 --> '\\n'\n",
      "\tToken ID 13 --> '\\n'\n",
      "\n",
      "#################### End of Llama2 Prompt Symbols ####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify token IDs for Llama2 prompt symbols\n",
    "print(f\"{stdout_padding} Llama2 Prompt Symbols {stdout_padding}\\n\")\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\" # Begin of instruction and end of instruction symbols\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<<SYS>>\\n\\n\" # Begin of system message and end of system message symbols\n",
    "\n",
    "prompt_symbols = [B_INST, E_INST, B_SYS, E_SYS]\n",
    "\n",
    "for symbol in prompt_symbols:\n",
    "    encoded_symbol = tokenizer.encode(symbol)\n",
    "    print(f'Encoded {repr(symbol)} becomes: {encoded_symbol}')\n",
    "    \n",
    "    for token in encoded_symbol:\n",
    "        print(f\"\\tToken ID {token} --> {repr(tokenizer.decode(token))}\")\n",
    "\n",
    "print(f\"\\n{stdout_padding} End of Llama2 Prompt Symbols {stdout_padding}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19021cd",
   "metadata": {},
   "source": [
    "### Note\n",
    "- \\<\\<SYS>>\\n , \\n\\<\\</SYS>\\n\\n :The beginning of system message and end of system message symbols\n",
    "- [INST] , [/INST] : The beginning of instruction and end of instruction symbols\n",
    "- Some words are broken as words and some as phrases.\n",
    "- Every token has a beginning of sentence encoded (bos) but not eos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45525627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Tokenizer Sentence Example ####################\n",
      "\n",
      "Original sentence: RHEL subscription manager let's you manage packages on RedHat.\n",
      "Encoded sentence: [1, 390, 29950, 6670, 25691, 8455, 1235, 29915, 29879, 366, 10933, 9741, 373, 4367, 29950, 271, 29889]\n",
      "Token ID 1 --> <s>\n",
      "Token ID 390 --> R\n",
      "Token ID 29950 --> H\n",
      "Token ID 6670 --> EL\n",
      "Token ID 25691 --> subscription\n",
      "Token ID 8455 --> manager\n",
      "Token ID 1235 --> let\n",
      "Token ID 29915 --> '\n",
      "Token ID 29879 --> s\n",
      "Token ID 366 --> you\n",
      "Token ID 10933 --> manage\n",
      "Token ID 9741 --> packages\n",
      "Token ID 373 --> on\n",
      "Token ID 4367 --> Red\n",
      "Token ID 29950 --> H\n",
      "Token ID 271 --> at\n",
      "Token ID 29889 --> .\n",
      "#################### End of Llama2 Tokenizer Sentence Example ####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try with custom sentence:\n",
    "# Test tokenizer on a sentence\n",
    "print(f\"{stdout_padding} Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")\n",
    "sentence = \"RHEL subscription manager let's you manage packages on RedHat.\"\n",
    "encoded_output = tokenizer.encode(sentence)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded sentence: {encoded_output}\")\n",
    "\n",
    "# Verify what each token ID correlates to\n",
    "for token in encoded_output:\n",
    "    print(f\"Token ID {token} --> {tokenizer.decode(token)}\")\n",
    "\n",
    "print(f\"{stdout_padding} End of Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "983d3113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Tokenizer Sentence Example ####################\n",
      "\n",
      "Original sentence: Can you add 2+2?\n",
      "Encoded sentence: [1, 1815, 366, 788, 29871, 29906, 29974, 29906, 29973]\n",
      "Token ID 1 --> <s>\n",
      "Token ID 1815 --> Can\n",
      "Token ID 366 --> you\n",
      "Token ID 788 --> add\n",
      "Token ID 29871 --> \n",
      "Token ID 29906 --> 2\n",
      "Token ID 29974 --> +\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29973 --> ?\n",
      "#################### End of Llama2 Tokenizer Sentence Example ####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try for numerical prompts\n",
    "# Test tokenizer on a sentence\n",
    "print(f\"{stdout_padding} Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")\n",
    "sentence = \"Can you add 2+2?\"\n",
    "encoded_output = tokenizer.encode(sentence)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded sentence: {encoded_output}\")\n",
    "\n",
    "# Verify what each token ID correlates to\n",
    "for token in encoded_output:\n",
    "    print(f\"Token ID {token} --> {tokenizer.decode(token)}\")\n",
    "\n",
    "print(f\"{stdout_padding} End of Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9159674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Tokenizer Sentence Example ####################\n",
      "\n",
      "Original sentence: Can you add 2222 + 2222?\n",
      "Encoded sentence: [1, 1815, 366, 788, 29871, 29906, 29906, 29906, 29906, 718, 29871, 29906, 29906, 29906, 29906, 29973]\n",
      "Token ID 1 --> <s>\n",
      "Token ID 1815 --> Can\n",
      "Token ID 366 --> you\n",
      "Token ID 788 --> add\n",
      "Token ID 29871 --> \n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 718 --> +\n",
      "Token ID 29871 --> \n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29973 --> ?\n",
      "#################### End of Llama2 Tokenizer Sentence Example ####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with < 4 digit numbers\n",
    "# Test tokenizer on a sentence\n",
    "print(f\"{stdout_padding} Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")\n",
    "sentence = \"Can you add 2222 + 2222?\"\n",
    "encoded_output = tokenizer.encode(sentence)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded sentence: {encoded_output}\")\n",
    "\n",
    "# Verify what each token ID correlates to\n",
    "for token in encoded_output:\n",
    "    print(f\"Token ID {token} --> {tokenizer.decode(token)}\")\n",
    "\n",
    "print(f\"{stdout_padding} End of Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb063f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Llama2 Tokenizer Sentence Example ####################\n",
      "\n",
      "Original sentence: Do you know the number 222222222222222\n",
      "Encoded sentence: [1, 1938, 366, 1073, 278, 1353, 29871, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906, 29906]\n",
      "Token ID 1 --> <s>\n",
      "Token ID 1938 --> Do\n",
      "Token ID 366 --> you\n",
      "Token ID 1073 --> know\n",
      "Token ID 278 --> the\n",
      "Token ID 1353 --> number\n",
      "Token ID 29871 --> \n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "Token ID 29906 --> 2\n",
      "#################### End of Llama2 Tokenizer Sentence Example ####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try with even bigger numbers\n",
    "num = 50\n",
    "# Test tokenizer on a sentence\n",
    "print(f\"{stdout_padding} Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")\n",
    "sentence = \"Do you know the number 50 *{22222222222222\"\n",
    "encoded_output = tokenizer.encode(sentence)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded sentence: {encoded_output}\")\n",
    "\n",
    "# Verify what each token ID correlates to\n",
    "for token in encoded_output:\n",
    "    print(f\"Token ID {token} --> {tokenizer.decode(token)}\")\n",
    "\n",
    "print(f\"{stdout_padding} End of Llama2 Tokenizer Sentence Example {stdout_padding}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd66e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-dv",
   "language": "python",
   "name": "python-dv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
