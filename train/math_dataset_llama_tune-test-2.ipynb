{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabcd970",
   "metadata": {},
   "source": [
    "# Fine Tuning with MATH dataset - llama 2 chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4407dfc",
   "metadata": {},
   "source": [
    "### Set up the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca0ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 12:04:02 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              43W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           Off | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              42W / 300W |     99MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           Off | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   52C    P0             101W / 300W |  31245MiB / 32768MiB |     74%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           Off | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             151W / 300W |  24961MiB / 32768MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           Off | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             151W / 300W |  24943MiB / 32768MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           Off | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   59C    P0             222W / 300W |  20055MiB / 32768MiB |     90%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           Off | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   49C    P0             129W / 300W |  17151MiB / 32768MiB |     49%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           Off | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              42W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    2   N/A  N/A   3465151      C   python                                    31242MiB |\n",
      "|    3   N/A  N/A    718663      C   python3                                   24950MiB |\n",
      "|    4   N/A  N/A    720155      C   python3                                   24932MiB |\n",
      "|    5   N/A  N/A    684755      C   python                                    20052MiB |\n",
      "|    6   N/A  N/A   4180359      C   python                                    17148MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63c5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu = os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b528f0c",
   "metadata": {},
   "source": [
    "### Hugging Face Login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92818c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a76fb",
   "metadata": {},
   "source": [
    "### Import and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a8d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/barman/crypto-llm-v2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-01 12:04:15.015548: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-01 12:04:15.066488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-01 12:04:16.271046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import accelerate\n",
    "# from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b2fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "#Force garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4dc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset from hugging_face\n",
    "math_dataset = load_dataset(\"hendrycks/competition_math\",trust_remote_code=True, split= \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea107118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'level', 'type', 'solution'],\n",
       "    num_rows: 7500\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dceacea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'Let \\\\[f(x) = \\\\left\\\\{\\n\\\\begin{array}{cl} ax+3, &\\\\text{ if }x>2, \\\\\\\\\\nx-5 &\\\\text{ if } -2 \\\\le x \\\\le 2, \\\\\\\\\\n2x-b &\\\\text{ if } x <-2.\\n\\\\end{array}\\n\\\\right.\\\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).', 'level': 'Level 5', 'type': 'Algebra', 'solution': 'For the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$. For example, $ax+3$ and $x-5$ must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\\\Rightarrow a=-3$. Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $a+b=-3+3=\\\\boxed{0}$.'}\n"
     ]
    }
   ],
   "source": [
    "# See how one sample looks like\n",
    "print(math_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d0fec",
   "metadata": {},
   "source": [
    "### Format the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d3e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What happens when we don't give this system prompt\n",
    "# TODO 2: Tweak the system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a fine-tuned AI model who is a math genious. \n",
    "You can solve simple to moderate level mathematics problems. \n",
    "Follow a chain of thought approach while answering and answer in brief.\"\"\"\n",
    "\n",
    "## Options\n",
    "sys_prompt = \"\"\"You are a math genious. Answer the following question on basic and advanced mathematics. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5859d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Check the question answer format\n",
    "def format_conversation_llama2(dataset):\n",
    "    '''\n",
    "    Formats a conversation in LLAMA2 style.\n",
    "\n",
    "    This function takes a dataset containing a problem and its solution, and formats it into a LLAMA2-style \n",
    "    conversation.\n",
    "\n",
    "    Args:\n",
    "    - dataset (dict): A dictionary containing the problem and solution of the conversation.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the formatted conversation.\n",
    "\n",
    "    Example:\n",
    "    >>> dataset = {'problem': 'How can I improve my coding skills?', 'solution': 'You can improve your coding skills \n",
    "    by practicing regularly and working on challenging projects.'}\n",
    "    >>> formatted_conversation = format_conversation_llama2(dataset)\n",
    "    >>> print(formatted_conversation)\n",
    "    {'text': '<s>[INST] <<SYS>> How can I improve my coding skills? <</SYS>> You can improve your coding skills by \n",
    "    practicing regularly and working on challenging projects. </s>'}\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    template = \"\"\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{question}[/INST] {answer}</s>\"\"\"\n",
    "    conversation = template.format(\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        question=dataset['problem'],\n",
    "        answer=dataset['solution'],\n",
    "    )\n",
    "\n",
    "    return {\"text\": conversation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "786f1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_format_dataset = math_dataset.map(\n",
    "    format_conversation_llama2,\n",
    "    remove_columns=math_dataset.column_names, # remove all columns; only \"text\" will be left\n",
    "    num_proc=os.cpu_count()  # multithreaded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da93e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<s>[INST] <<SYS>>\\nYou are a fine-tuned AI model who is a math genious. \\nYou can solve simple to moderate level mathematics problems. \\nFollow a chain of thought approach while answering and answer in brief.\\n<</SYS>>\\n\\nWhat is the degree of the polynomial $(4 +5x^3 +100 +2\\\\pi x^4 + \\\\sqrt{10}x^4 +9)$?[/INST] This polynomial is not written in standard form.  However, we don't need to write it in standard form, nor do we need to pay attention to the coefficients.  We just look for the exponents on $x$.  We have an $x^4$ term and no other term of higher degree, so $\\\\boxed{4}$ is the degree of the polynomial.</s>\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_format_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715872c",
   "metadata": {},
   "source": [
    "### Define the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f139295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned math model name (date-month-hour-minutes)\n",
    "new_model = \"llama-2-7b-chat-math-01-04-12-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f40b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output directory where the model predictions,configuration files and checkpoints will be stored\n",
    "output_dir = f\"./results/{new_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee22bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/llama-2-7b-chat-math-01-04-12-10/tokenizer_config.json',\n",
       " './results/llama-2-7b-chat-math-01-04-12-10/special_tokens_map.json',\n",
       " './results/llama-2-7b-chat-math-01-04-12-10/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 3: Check the padding\n",
    "# Load LLaMA tokenizer\n",
    "# set_special_tokens = True, so the bos (beginning of sequence </s>) and eos (end of sequence </s>) token is added\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, set_special_tokens = True,  trust_remote_code=True)\n",
    "\n",
    "# not adding mask token for now - can be used to give attention to some part of the sequence\n",
    "# manually add the padding token and set it to the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6424458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e177787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f066d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# use_flash_attention = False\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 5\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "# Logging Directory\n",
    "logging_dir = f\"./logs/{new_model}/\"\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0 (or the only visible GPU)\n",
    "device_map = {\"\": 0}\n",
    "# TODO : Try setting it to a number instead of auto\n",
    "# device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac9a283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dfc16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    cache_dir=\"/projects/barman/cache\",\n",
    "#     use_flash_attention_2=use_flash_attention,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "    modules_to_save= [\"embed_tokens\", \"lm_head\"]\n",
    ")\n",
    "\n",
    "# View the model layers\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7bd07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    #logging_dir= logging_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "#     load_best_model_at_end=True, # This option only saves the best model and no checkpoints\n",
    "    #save_strategy=\"no\", # use this when using the load_best_model_at_end (requires save_strategy and eval_strategy to be same)\n",
    "    #evaluation_strategy=\"epoch\", \n",
    "    #logging_strategy=\"epoch\",\n",
    "    \n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=math_format_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0d523ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.to_json_file(output_dir + \"/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9f5ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if needed\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb26ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/9375 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model - default directory output directory \n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d32d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "loss_outputs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Select desired columns\n",
    "desired_columns = [\"loss\", \"learning_rate\", \"epoch\", \"step\"]\n",
    "\n",
    "# Keep only those columns in the DataFrame\n",
    "loss_outputs = loss_outputs[desired_columns]\n",
    "\n",
    "# Save the loss results\n",
    "filename = f\"loss-results\"\n",
    "filepath = os.path.join(f\"results/{new_model}\", filename)  # Replace with your desired path\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "loss_outputs.to_csv(filepath, index=False)  # Avoid saving index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Use use_reentrant= True or False explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"./results/{new_model}/runs/\"\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "# log_dir = f\"./results/{new_model}/runs/\"\n",
    "# notebook.start(\"--log_dir {} --port 4000\".format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./results/llama-2-7b-chat-math-27-3-16-20/runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bf52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# prompt = \"Do you know piecewise linear function?\"\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "# result = pipe(f\"<s>[INST]  <<SYS>> {DEFAULT_SYSTEM_PROMPT} <</SYS>> {prompt} [/INST]\")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987381d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ee7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33eaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7f7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa4d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-llm-v2-python",
   "language": "python",
   "name": "crypto-llm-v2-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
