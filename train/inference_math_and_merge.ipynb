{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadb863e",
   "metadata": {},
   "source": [
    "## Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc42b5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 10:41:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              43W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           Off | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              57W / 300W |  14569MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           Off | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   53C    P0             174W / 300W |  31245MiB / 32768MiB |     78%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           Off | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             151W / 300W |  24961MiB / 32768MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           Off | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             149W / 300W |  24943MiB / 32768MiB |     86%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           Off | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   59C    P0             187W / 300W |  20055MiB / 32768MiB |     89%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           Off | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   48C    P0              75W / 300W |  17151MiB / 32768MiB |     27%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           Off | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A   4149777      C   ...cts/barman/crypto-llm-v2/bin/python    12086MiB |\n",
      "|    2   N/A  N/A   3465151      C   python                                    31242MiB |\n",
      "|    3   N/A  N/A    718663      C   python3                                   24950MiB |\n",
      "|    4   N/A  N/A    720155      C   python3                                   24932MiB |\n",
      "|    5   N/A  N/A    684755      C   python                                    20052MiB |\n",
      "|    6   N/A  N/A   4180359      C   python                                    17148MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2805e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu = os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da676316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer,pipeline, logging, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4f1f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce137b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned math model name (date-month-hour-minutes)\n",
    "new_model = \"llama-2-7b-chat-math-27-3-16-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0e63513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = f\"./results/{new_model}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ec6f884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results/llama-2-7b-chat-math-27-3-16-20/'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3fc1eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6cbc3750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load finetuned LLM model and tokenizer\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    cache_dir=\"/projects/barman/cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a0f76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5595b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f89aefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b1cdb535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    cache_dir=\"/projects/barman/cache\",\n",
    "#     use_flash_attention_2=use_flash_attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3be60cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset from hugging_face\n",
    "math_dataset = load_dataset(\"hendrycks/competition_math\",trust_remote_code=True, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3618bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = [data for data in math_dataset if data['level']==\"Level 1\" or data['level']==\"Level 2\"]\n",
    "sample =  filtered_dataset[randrange(len(filtered_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfb9c8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'A scale drawing of a park shows that one inch represents 800 feet. A line segment in the drawing that is 4.75 inches long represents how many feet?',\n",
       " 'level': 'Level 1',\n",
       " 'type': 'Algebra',\n",
       " 'solution': 'Each inch of the 4.75-inch line segment represents 800 feet, so the whole line segment represents $4.75\\\\times800=\\\\frac{19}{4}\\\\cdot800=19\\\\cdot200=\\\\boxed{3800}$ feet.'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d937f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a fine-tuned AI model who is a math genious. \n",
    "You can solve simple to moderate level mathematics problems. \n",
    "Follow a chain of thought approach while answering. Answer in brief. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb31bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "You are a fine-tuned AI model who is a math genious. \n",
      "You can solve simple to moderate level mathematics problems. \n",
      "Follow a chain of thought approach while answering. Answer in brief. \n",
      "\n",
      "Input:\n",
      "A scale drawing of a park shows that one inch represents 800 feet. A line segment in the drawing that is 4.75 inches long represents how many feet?\n",
      "\n",
      "Generated Response with fine tuned model:\n",
      "   Great, let's get started!\n",
      "\n",
      "Given that one inch in the drawing represents 800 feet, and the line segment is 4.75 inches long, we can use the conversion factor to find the corresponding length in feet.\n",
      "\n",
      "So, if one inch represents 800 feet, then the length of the line segment in feet can be calculated as:\n",
      "\n",
      "4.75 inches Ã— 800 feet/inch = 3600 feet\n",
      "\n",
      "Therefore, the line segment in the drawing represents 3600 feet.\n",
      "\n",
      "Generated Response with the base model:\n",
      "   Great, let's solve this problem together! ðŸ¤”\n",
      "\n",
      "So, we know that in this scale drawing, one inch represents 800 feet. That means that if we want to find the length of a line segment in the drawing that is 4.75 inches long, we can simply multiply the length of the segment in the drawing by the scale factor of 1:800. ðŸ’¡\n",
      "\n",
      "So, the length of the line segment in real life that corresponds to the 4.75 inches in the drawing would be:\n",
      "\n",
      "4.75 inches x 800 feet/inch = 3,600 feet\n",
      "\n",
      "Therefore, the line segment in the drawing represents 3,600 feet in real life. ðŸ“\n",
      "\n",
      "How's that? Did I help you solve the problem? ðŸ˜Š\n",
      "\n",
      "Ground Truth:\n",
      "Each inch of the 4.75-inch line segment represents 800 feet, so the whole line segment represents $4.75\\times800=\\frac{19}{4}\\cdot800=19\\cdot200=\\boxed{3800}$ feet.\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "# Run text generation pipeline with our next model\n",
    "# prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=ft_model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] <<SYS>> {DEFAULT_SYSTEM_PROMPT} <</SYS>> {sample['problem']} [/INST]\")\n",
    "\n",
    "pipe2 = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=2048)\n",
    "result2 = pipe2(f\"<s>[INST] <<SYS>> {DEFAULT_SYSTEM_PROMPT} <</SYS>> {sample['problem']} [/INST]\")\n",
    "\n",
    "print(f\"Instruction:\\n{DEFAULT_SYSTEM_PROMPT}\\n\")\n",
    "print(f\"Input:\\n{sample['problem']}\\n\")\n",
    "print(f\"Generated Response with fine tuned model:\\n {result[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")\n",
    "print(f\"Generated Response with the base model:\\n {result2[0]['generated_text'].split(\"[/INST]\")[-1]}\\n\")\n",
    "print(f\"Ground Truth:\\n{sample['solution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f127b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-llm-v2-python",
   "language": "python",
   "name": "crypto-llm-v2-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
